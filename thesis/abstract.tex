% !TEX root = ./main.tex

Imagine a robot navigating a complex and unfamiliar environment -
underwater, subterranean, or even a dilapidated building.  Current
Visual Place Recognition (VPR) techniques often struggle with such
diverse scenarios, requiring re-training for each new environment.
This limitation hinders the development of truly autonomous robots.
This work introduces AnyLoc, a novel VPR system taking a significant
step towards universality.  AnyLoc leverages feature representations
learned by powerful foundation models, eliminating the need for
VPR-specific training.  Furthermore, by combining these features with
unsupervised aggregation techniques, AnyLoc can uncover unique visual
characteristics that define different environments.  Our experiments
demonstrate AnyLoc's potential to function across multiple
environments (outdoor, indoor, aerial, underwater, subterranean, and
dilapidated) without retraining, laying the groundwork for VPR
solutions that could be deployed anywhere, anytime, and across
anyview.
