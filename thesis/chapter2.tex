% !TEX root = ./main.tex

All the basics of Vision Foundation Models required for understanding 
this thesis.

Foundation models (virtually all AI modes in general) have the 
following components
\begin{itemize}
    \item \emph{Model architecture}: MLP, convolution, transformers. 
        Also MLP mixer \cite{Tolstikhin2021MLPMixerAA}, ConvNext 
        \cite{Liu2022ACF}, transformer variants (CCT) 
        \cite{Hassani2021EscapingTB}, etc. 
    \item \emph{Dataset}: type (labelled for supervised, unlabelled 
        for unsupervised or self-supervised), size (large), 
        augmentations, data processing pipelines. 
    \item \emph{Objective, training strategy and Loss function}: 
        formulation of training procedure to guide the model output. 
        Distillation \cite{Hinton2015DistillingTK}, representation 
        learning, MAE \cite{He2021MaskedAA}, contrastive losses 
        (aligning modalities), knowledge transfer (student-teacher), 
        MoCo \cite{He2019MomentumCF, Chen2020ImprovedBW}, SwAV 
        \cite{Caron2020UnsupervisedLO}, SimCLR
        \cite{Chen2020ASF, Chen2020BigSM}, 
        BYOL \cite{Grill2020BootstrapYO}, etc.
    \item \emph{Optimizer}: usually Adam \cite{Kingma2014AdamAM} 
        (doesn't need explanation)
\end{itemize}

\section{Vision Transformers}

ViT \cite{Dosovitskiy2020AnII} and DeIT \cite{Touvron2020TrainingDI}

\section{SSL Concepts}

Start with a short summary of the SSL cookbook \cite{Balestriero2023ACO}.

Some of the above along with requirements for DINOv2: 
iBOT \cite{Zhou2021iBOTIB}, LayerScale and Stochastic Depth 
\cite{Huang2016DeepNW}, KoLeo regularizer 
\cite{Sablayrolles2018SpreadingVF}, SwiGLU activation 
\cite{Shazeer2020GLUVI}, Sinkhorn-Knoop centering 
\cite{Caron2020UnsupervisedLO} (SwAV).

\section{DINO and DINOv2 details}

Architecture, data, training, etc.
